{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Media / Twitter Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to get to know what the Media is speaking about, in order to do this, I will extract the information from the main Digital Newspapers in Argentina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import bs4 as bs\n",
    "from urllib import urlopen as uReq\n",
    "import sys\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La Nación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I request the URL, read, parsed with BS and saved it in the variable soup\n",
    "\n",
    "# I create the data_ranges and set the dates for which I will scrap the data\n",
    "date_range = pd.date_range('2017-04-28',periods=8, freq='d')\n",
    "date_range = date_range.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "\n",
    "# creates the file, using dd/mm/yyyy in the filename\n",
    "filename = '/users/hnesman/Downloads/DATA_Science/La Nacion.txt'\n",
    "    # opens the file\n",
    "f = open(filename,'w')\n",
    "    # creates and writes the headers of the file\n",
    "headers = \"Date|Sección|Title|Encabezado\\n\"\n",
    "f.write(headers)\n",
    "\n",
    "# For each day, I have to iterate over Section, so I create the list of sections, I use the url code for each section\n",
    "secciones = ['c30','c272','c7','c28','c131','c337','c432','c7775']\n",
    "\n",
    "# I will use this map later for labeling each article\n",
    "# sec_dic = {'c30':'política','c272':'economía','c7':'elmundo','c28':'opinión','c131':'deportiva','c337':'elcampo',\n",
    "#           'c432':'tecnología','c7775':'seguridad'}\n",
    "\n",
    "# Loop over each section within each day\n",
    "for i in date_range:\n",
    "        for sec in secciones:\n",
    "        # creates de url link with the date and the section\n",
    "            url = 'http://servicios.lanacion.com.ar/archivo-f%s-%s' % (i,sec)\n",
    "            # creates de BS object from the URL\n",
    "            sauce = uReq(url)\n",
    "            # does the parsing\n",
    "            soup = bs.BeautifulSoup(sauce,'html.parser')\n",
    "            \n",
    "            # gets the tabs containing the date from the BS object, in this case I take the date from the url\n",
    "            date = soup.findAll('div',{'class':'titFecha'})\n",
    "            # converts date into string \n",
    "            date = str(date)\n",
    "            # this is to extract the date from the date container\n",
    "            fecha = '\"titFecha\">'\n",
    "            date = date[date.find(fecha)+len(fecha):date.find(fecha)+21]\n",
    "            # slices date to get dd/mm/yyyy\n",
    "            day = date[:2]\n",
    "            month = date[3:5]\n",
    "            year = date[6:12]\n",
    "            date = (day+'/'+month+'/'+year)\n",
    "            print date + ' - ' + sec\n",
    "\n",
    "            \n",
    "            # creates the list of containers of the data I want from the 'url'\n",
    "            acumulados = soup.findAll('li',{'class':'acumulados'})\n",
    "            \n",
    "            #creates the loop for each element of the list acumulados\n",
    "            for acum in acumulados:\n",
    "                # gets the title (which is the url link)\n",
    "                title =  acum.a['href']\n",
    "                print title\n",
    "                # gets the body of the element\n",
    "                encabezado = acum.p\n",
    "                print encabezado,'\\n'\n",
    "                #writes the info (date, section, title & encabezado) into the file\n",
    "                f.write(str(date) + '|'+ str(sec) + '|' + str(title) + '|' + str(encabezado).replace('<p>','').replace('</p>','') + '\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open the file and put into a dataFrame\n",
    "LaNacion = pd.read_table('/users/hnesman/Downloads/DATA_Science/La Nacion.txt',delimiter=\"|\")\n",
    "\n",
    "LaNacion.columns = ['Date', 'SecCode','Title','Encabezado']\n",
    "\n",
    "# Creates the Dictionary that I will use to map the Section Codes from the 'url's\n",
    "sec_dic = {'c30':'política','c272':'economía','c7':'elmundo','c28':'opinión','c131':'deportiva','c337':'elcampo',\n",
    "          'c432':'tecnología','c7775':'seguridad'}\n",
    "# Do the mapping\n",
    "LaNacion['Sec Label'] = LaNacion.SecCode.apply(lambda x: sec_dic[str(x)])\n",
    "\n",
    "# Add 'Media' column, which is the label for the name of the Newspaper\n",
    "LaNacion['Media'] = 'La Nacion'\n",
    "\n",
    "# Drop the column 'SecCode'\n",
    "LaNacion = LaNacion.drop('SecCode',axis=1)\n",
    "\n",
    "# Set the names of the columns\n",
    "LaNacion.columns = ['Date','Title','Encabezado','SecLabel','Media']\n",
    "\n",
    "# Save to .csv file\n",
    "LaNacion.to_csv('/users/hnesman/Downloads/DATA_Science/LaNacion.csv')\n",
    "LaNacion.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La Voz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the DataFrame for the dataset\n",
    "date_range = pd.date_range('2017-4-28',periods=8, freq='d')\n",
    "date_range = date_range.strftime(\"%Y%m%d\")\n",
    "\n",
    "# Create and open the file, and create the 'Headers' on it\n",
    "filename = '/users/hnesman/Downloads/DATA_Science/LaVoz.txt'\n",
    "f = open(filename,'w')\n",
    "headers = \"Date| Title| Encabezado | SecLabel \\n\"\n",
    "f.write(headers)\n",
    "\n",
    "# Iterate over each date on the 'date_range' and access each 'url' for scraping the webpage\n",
    "for i in date_range:\n",
    "    url = 'http://www.lavoz.com.ar/ediciones_anteriores?fecha=%s' %i\n",
    "    \n",
    "    # Format the date\n",
    "    date = i\n",
    "    day = date[6:9]\n",
    "    month = date[4:6]\n",
    "    year = date[:4]\n",
    "    date = (day+'/'+month+'/'+year)\n",
    "    print date\n",
    "    \n",
    "    # Request the url data and parse it using BeautifulSoup\n",
    "    sauce = uReq(url)\n",
    "        # does the parsing\n",
    "    soup = bs.BeautifulSoup(sauce,'html.parser')\n",
    "       # Gets each (all) the sections for the date 'i' \n",
    "    noticias = soup.find_all('div',{'class':'contenido'})\n",
    "    print len(noticias)\n",
    "    \n",
    "    # Loops over each article within 'noticias\n",
    "    for i in noticias:\n",
    "        # Gets the title, that includes the Section of the news, and the title. I need to split it. (I could try go get the body of the news) \n",
    "        title = i.a['href']\n",
    "        empty, secLabel, title = title.split('/')\n",
    "        #I will copy the values of 'title' to 'encabezado' (header) \n",
    "        encabezado = title\n",
    "        print i.a['href'], '\\n'\n",
    "        # I write in the file, the values I took from the webpage\n",
    "        f.write(date +'|'+ encabezado.replace('-',' ') +'|'+ title.replace('-',' ') + '|' + secLabel + '\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open the file and put into a dataFrame\n",
    "LaVoz = pd.read_table('/users/hnesman/Downloads/DATA_Science/LaVoz.txt',delimiter=\"|\")\n",
    "\n",
    "# Add 'Media' column, which is the label for the name of the Newspaper\n",
    "LaVoz['Media'] = 'La Voz'\n",
    "\n",
    "# Set the names of the columns\n",
    "LaVoz.columns = ['Date','Title','Encabezado','SecLabel','Media']\n",
    "\n",
    "# Save to .csv file\n",
    "LaVoz.to_csv('/users/hnesman/Downloads/DATA_Science/LaVoz.csv')\n",
    "LaVoz.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Los Andes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create and open the file, and set the 'Headers' on the file\n",
    "filename = '/users/hnesman/Downloads/DATA_Science/LosAndes_.txt'\n",
    "f = open(filename,'w')\n",
    "headers = \"Date|Title|Encabezado|SecLabel|\\n\"\n",
    "f.write(headers)\n",
    "\n",
    "# Set the period for the analysis\n",
    "date = '20170428'\n",
    "periods = 8\n",
    "\n",
    "# Set the date format I need to use in 'url'\n",
    "fecha = pd.date_range(date , periods = periods, freq = 'D')\n",
    "fecha=[x.strftime('%Y-%m-%d').split('-') for x in fecha ]\n",
    "fecha=['-'.join([x[0],x[1].lstrip('0'),x[2].lstrip('0')]) for x in fecha]\n",
    "\n",
    "# Access each url ('date'), and scrap the data I need - Using BeautifulSoup for parsing 'html\n",
    "for i in fecha:\n",
    "    \n",
    "    url = 'http://losandes.com.ar/titulos/%s' %i\n",
    "    sauce = uReq(url)\n",
    "        # does the parsing\n",
    "    soup = bs.BeautifulSoup(sauce,'html.parser')\n",
    "        # gets the tabs containing the date from the BS object\n",
    "\n",
    "    seccion = soup.findAll('section',{'class':'secciondiario'})\n",
    "    \n",
    "    # Define a variable for each section\n",
    "    economia= seccion[0]\n",
    "    mundo = seccion[1]\n",
    "    politica = seccion[3]\n",
    "    deportes = seccion[4]\n",
    "    policiales = seccion[5]\n",
    "    \n",
    "    # Prepare the format for the variable 'Date'\n",
    "    date = i\n",
    "    print date\n",
    "    year, month, day = date.split('-')\n",
    "\n",
    "    # I work on the format of the dates (day / month)\n",
    "    month = '0' + month\n",
    "    \n",
    "    if day in ['1','2','3','4','5','6','7','8','9']:\n",
    "        day = '0' + day\n",
    "    else:\n",
    "        day = day\n",
    "    date = day + '/' + month + '/' + year\n",
    "    \n",
    "    # Get all the tags 'li' within the each Section, and from each 'li' I get the tag 'a', and the write on the file\n",
    "    # cleaning some html code\n",
    "    not_pol = politica.findAll('li')\n",
    "    for i in not_pol:\n",
    "        title = i.a\n",
    "        f.write(date +'|' + str(title).replace('<a href=\"/article/','').replace('</a>','').replace('-',' ') + '|'+ '|'+ 'Política' + '\\n')\n",
    "\n",
    "    not_dep = deportes.findAll('li')\n",
    "    for i in not_dep:\n",
    "        title = i.a\n",
    "        f.write(date + '|' + str(title).replace('<a href=\"/article/','').replace('</a>','').replace('-',' ') +'|'+ '|'+ 'Deportes' + '\\n')\n",
    "\n",
    "    not_econ = economia.findAll('li')\n",
    "    for i in not_econ:\n",
    "        title = i.a\n",
    "        f.write(date + '|' + str(title).replace('<a href=\"/article/','').replace('</a>','').replace('-',' ') + '|'+ '|'+ 'Economia' + '\\n')\n",
    "    \n",
    "    not_mundo = mundo.findAll('li')\n",
    "    for i in not_m:\n",
    "        title = i.a\n",
    "        f.write(date + '|' + str(title).replace('<a href=\"/article/','').replace('</a>','').replace('-',' ') +'|'+ '|'+ 'Mundo' + '\\n')\n",
    "    \n",
    "    not_polic = policiales.findAll('li')\n",
    "    for i in noticia:\n",
    "        title = i.a\n",
    "        f.write(date + '|' + str(title).replace('<a href=\"/article/','').replace('</a>','').replace('-',' ') +'|'+ '|'+ 'Policiales' +  '\\n')\n",
    "           \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open the file and put into a dataFrame\n",
    "LosAndes = pd.read_table('/users/hnesman/Downloads/DATA_Science/losAndes_.txt',delimiter=\"|\")\n",
    "\n",
    "# Change names in Headers, and add 'Media' column, which is the label for the name of the Newspaper\n",
    "LosAndes['Encabezado'] = LosAndes['Title']\n",
    "LosAndes['Media'] = 'Los Andes'\n",
    "# I drop a column that was created during the opening of the .txt file #### ATTENTION -  didn't happen in LaVoz, check!\n",
    "LosAndes = LosAndes.drop('Unnamed: 4', axis =1)\n",
    "# Set the names of the columns\n",
    "LosAndes.columns = ['Date','Title','Encabezado','SecLabel','Media']\n",
    "# Save to .csv file\n",
    "LosAndes.to_csv('/users/hnesman/Downloads/DATA_Science/LosAndes_.csv',encoding='utf-8')\n",
    "LosAndes.head()\n",
    "LosAndes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I have to merge the datasets for each Media, into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Opening the files into dataFrames\n",
    "LosAndes = pd.read_csv('/users/hnesman/Downloads/DATA_Science/LosAndes.csv')\n",
    "LaNacion = pd.read_csv('/users/hnesman/Downloads/DATA_Science/LaNacion.csv')\n",
    "LaVoz = pd.read_csv('/users/hnesman/Downloads/DATA_Science/LaVoz.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Number of Articles per Newspaper\n",
    "print len(LosAndes)\n",
    "print len(LaNacion)\n",
    "print len(LaVoz)\n",
    "\n",
    "\n",
    "# Do a plot\n",
    "media_len = [len(LosAndes),len(LaNacion), len(LaVoz)]\n",
    "Newspaper = [str('LosAndes'),str('LaNacion'), str('LaVoz')]\n",
    "\n",
    "df = pd.DataFrame({'media_len': media_len, 'Newspaper': Newspaper})\n",
    "df.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenating the dataFrames, drop unused columns and set the name of the 'columns'\n",
    "MediaNews = pd.concat([LosAndes,LaNacion])\n",
    "MediaNews = pd.concat([LaVoz,MediaNews])\n",
    "MediaNews = MediaNews.drop('Unnamed: 0',axis=1)\n",
    "MediaNews.columns = ['Date', 'Title', 'Encabezado', 'Art_Label', 'Media']\n",
    "len(MediaNews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inspection of the dataset and sorting\n",
    "print type(MediaNews)\n",
    "print MediaNews.columns\n",
    "MediaNews = MediaNews.sort_values(by = ['Date','Art_Label'], axis = 0, ascending = True, inplace = False)\n",
    "MediaNews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the dictionary for unifying the values of 'Art_Label' and applying the funcion\n",
    "cmap = {'deportiva': 'deportes', 'Deportes': 'deportes', 'econom\\xc3\\xada': 'economia', 'economia': 'economia',\n",
    "        'Economia': 'economia', 'opini\\xc3\\xb3n': 'editorial', 'opinion': 'editorial',\n",
    "        'editorial': 'editorial', 'politica': 'politica', 'Pol\\xc3\\xadtica': 'politica', 'pol\\xc3\\xadtica': 'politica', 'tecnolog\\xc3\\xada': 'otros', 'elcampo': 'otros', 'sucesos': 'policiales', \n",
    "        'seguridad': 'policiales', 'Policiales': 'policiales', 'mundo': 'elmundo', 'Mundo': 'elmundo', 'elmundo': 'elmundo', 'negocios': 'otros','ciudadanos': 'otros'}\n",
    "\n",
    "MediaNews['Art_Label'] = MediaNews.Art_Label.apply(lambda x: cmap[str(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the file to .csv\n",
    "MediaNews.to_csv('/users/hnesman/Downloads/DATA_Science/MediaNews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the libraries for starting the Unsupervised Learning\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open the file into a dataFrame, clean a column \"Unnamed: 0' that is created everytime I open the .csv file\n",
    "\n",
    "data = pd.read_csv('/users/hnesman/Downloads/DATA_Science/MediaNews.csv', sep=',')\n",
    "data = data.drop('Unnamed: 0', axis=1)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I want to check how many articles for each Section and Media / Should do a Group By / I have problems with the labels\n",
    "\n",
    "df2 = data['Art_Label'].value_counts()\n",
    "df2.plot(kind = 'hist', xlim = (0,400))\n",
    "print df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now I have to prepare for doing the Word Vector routine\n",
    "\n",
    "1) I will set up a list of STOP_WORDS in Spanish, I will feed this list after a couple of outcomes of the Vectorization\n",
    "\n",
    "2) I do the CountVector, I will generate different combinations of 'NGrams', to identify any ambiguous context. (i.e.: papa, is Pope, and Potatoe in Spanish), this risk is increased by the factor of 'casual' writing in Twitter.\n",
    "\n",
    "3) Last stage, is a visual inspection of the n-NGrams lists created. \n",
    "\n",
    "4) I cut a final list of terms, based on my expertise. (Otherwise machines would do everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I set a list of STOP WORDS in Spanish\n",
    "# coding: utf8\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "\n",
    "STOP_WORDS = set(\"\"\" 2004 44 '44 url' 000 100 http https 'http co' 'https co' 'cont co' cont co\n",
    "none anos ano año años actualmente acuerdo adelante ademas además adrede afirmó agregó ahi ahora ahí\n",
    "al algo alguna algunas alguno algunos algún alli allí alrededor ambos ampleamos\n",
    "antano antaño ante anterior antes apenas aproximadamente aquel aquella aquellas\n",
    "aquello aquellos aqui aquél aquélla aquéllas aquéllos aquí arriba arribaabajo\n",
    "aseguró asi así atras aun aunque ayer añadió aún\n",
    "bajo bastante bien breve buen buena buenas bueno buenos\n",
    "cada casi cerca cierta ciertas cierto ciertos cinco claro comentó como con\n",
    "conmigo conocer conseguimos conseguir considera consideró consigo consigue\n",
    "consiguen consigues contigo contra cosas creo cual cuales cualquier cuando\n",
    "cuanta cuantas cuanto cuantos cuatro cuenta cuál cuáles cuándo cuánta cuántas\n",
    "cuánto cuántos cómo\n",
    "da dado dan dar de debajo debe deben debido decir dejó del delante demasiado\n",
    "demás dentro deprisa desde despacio despues después detras detrás dia dias dice\n",
    "dicen dicho dieron diferente diferentes dijeron dijo dio donde dos durante día\n",
    "días dónde\n",
    "ejemplo el ella ellas ello ellos embargo empleais emplean emplear empleas\n",
    "empleo en encima encuentra enfrente enseguida entonces entre era eramos eran\n",
    "eras eres es esa esas ese eso esos esta estaba estaban estado estados estais\n",
    "estamos estan estar estará estas este esto estos estoy estuvo está están ex\n",
    "excepto existe existen explicó expresó él ésa ésas ése ésos ésta éstas éste\n",
    "éstos\n",
    "fin final fue fuera fueron fui fuimos\n",
    "general gran grandes gueno\n",
    "ha haber habia habla hablan habló habrá había habían hace haceis hacemos hacen hacer\n",
    "hacerlo haces hacia haciendo hago han hasta hay haya he hecho hemos hicieron\n",
    "hizo horas hoy hubo\n",
    "igual incluso indicó informo informó intenta intentais intentamos intentan\n",
    "intentar intentas intento ir\n",
    "junto\n",
    "la lado largo las le lejos les llegó lleva llevar lo los luego lugar\n",
    "mal manera manifestó mas mayor me mediante medio mejor mencionó menos menudo mi\n",
    "mia mias mientras mio mios mis misma mismas mismo mismos modo momento mucha\n",
    "muchas mucho muchos muy más mí mía mías mío míos\n",
    "nada nadie ni ninguna ningunas ninguno ningunos ningún no nos nosotras nosotros\n",
    "nuestra nuestras nuestro nuestros nueva nuevas nuevo nuevos nunca\n",
    "ocho os otra otras otro otros\n",
    "pais para parece parte partir pasada pasado paìs peor pero pesar poca pocas\n",
    "poco pocos podeis podemos poder podria podriais podriamos podrian podrias podrá\n",
    "podrán podría podrían poner por porque posible primer primera primero primeros\n",
    "principalmente pronto propia propias propio propios proximo próximo próximos\n",
    "pudo pueda puede pueden puedo pues\n",
    "qeu que quedó queremos quien quienes quiere quiza quizas quizá quizás quién quiénes qué\n",
    "raras realizado realizar realizó repente respecto rt\n",
    "sabe sabeis sabemos saben saber sabes salvo se sea sean segun segunda segundo\n",
    "según seis ser sera será serán sería señaló si sido siempre siendo siete sigue\n",
    "siguiente sin sino sobre sois sola solamente solas solo solos somos son soy\n",
    "soyos su supuesto sus suya suyas suyo sé sí sólo\n",
    "tal tambien también tampoco tan tanto tarde te temprano tendrá tendrán teneis\n",
    "tenemos tener tenga tengo tenido tenía tercera ti tiempo tiene tienen toda\n",
    "todas todavia todavía todo todos total trabaja trabajais trabajamos trabajan\n",
    "trabajar trabajas trabajo tras trata través tres tu tus tuvo tuya tuyas tuyo\n",
    "tuyos tú\n",
    "ultimo un una unas uno unos usa usais usamos usan usar usas uso usted ustedes\n",
    "última últimas último últimos URL url\n",
    "va vais valor vamos van varias varios vaya veces ver verdad verdadera verdadero\n",
    "vez vosotras vosotros voy vuelve vuestra vuestras vuestro vuestros\n",
    "ya yo\n",
    "\"\"\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I create my Count Vectorizer in order to Tokenize the articles, I will create a couple CountVect for different\n",
    "# values of ngram_range(): (1,3) - (2-3) - (3,3). With them I will create dataFrames and inspect which are the\n",
    "# n-grams that I can use to search on Twitter.\n",
    "\n",
    "for i in range(1,4):\n",
    "\n",
    "    cv_i = CountVectorizer(binary = False, stop_words = STOP_WORDS, min_df = 3, ngram_range=(i, 3))\n",
    "\n",
    "    docs_i = cv_i.fit_transform(data.Encabezado.dropna())\n",
    "    id2word_i = dict(enumerate(cv_i.get_feature_names()))\n",
    "    if i == 1:\n",
    "        NGram_1 = pd.DataFrame({'index' : id2word_i.keys() , 'ngram' : unicode(id2word_i.values()) })\n",
    "    elif i == 2:\n",
    "        NGram_2 = pd.DataFrame({'index' : id2word_i.keys() , 'ngram' : unicode(id2word_i.values()) })\n",
    "    else:\n",
    "        NGram_3 = pd.DataFrame({'index' : id2word_i.keys() , 'ngram' : unicode(id2word_i.values()) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I copied the result of the CountVectorizer into .csv files so I can inspect by my own which 'tokens' I will\n",
    "# use for the Twitter Search\n",
    "\n",
    "NGram_1.to_csv('/users/hnesman/Downloads/DATA_Science/NGram_1.csv')\n",
    "NGram_2.to_csv('/users/hnesman/Downloads/DATA_Science/NGram_2.csv')\n",
    "NGram_3.to_csv('/users/hnesman/Downloads/DATA_Science/NGram_3.csv')\n",
    "\n",
    "# I need techniques to analyze this list with Python, originally I did it with Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I have a list of the most relevant terms acording to Media. I will check what people are saying regarding this terms in Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "\n",
    "try:\n",
    "    import json\n",
    "except ImportError:\n",
    "    import simplejson as json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I will make use of the Twitter APIs combined with a wrapper called Tweepy\n",
    "\n",
    "#### To use the Twitter APIs is necessary to get a set of credentials which are provided when you create an app in Twitter. Also the use of the APIs are limited by time and by number of calls, I created two apps, in order to get two set of credentials and speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I set up my credentials for getting access to Twitter API - App 1\n",
    "ckey=\"poglAcoBXr2U41V3BRerO8SoiX\"\n",
    "csecret=\"uo3MmdHBNQX5O5SV84N70IHkRCIPgOiVKNwIg1PQMznq1ZuvM8j\"\n",
    "atoken=\"t848659773176385536-XOsIpQcOurrAYHE8qjnq6yq2gtJpw01\"\n",
    "asecret=\"agIazV001cMvvqtPdfJ1XyksnwSrednnNYQMb1z44PysPb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I define the function 'search_tweets', then to do the query we have to specify a'term' and a 'date'.\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "\n",
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "import csv\n",
    "\n",
    "# Date as YYYY-MM-DD\n",
    "def search_tweets(query,date):\n",
    "\n",
    "\t#authorize twitter, initialize tweepy\n",
    "\tauth = tweepy.OAuthHandler(ckey, csecret)\n",
    "\tauth.set_access_token(atoken, asecret)\n",
    "\tapi = tweepy.API(auth)\n",
    "\t\n",
    "\t#initialize a list to hold all the tweepy Tweets\n",
    "\talltweets = []\t\n",
    "\t\n",
    "\t#make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "\tnew_tweets = api.search(q = query,count=10, result_type = 'recent', include_entities =  True)\n",
    "\t\n",
    "\t#save most recent tweets\n",
    "\talltweets.extend(new_tweets)\n",
    "\t\n",
    "\t#save the id of the oldest tweet less one\n",
    "\toldest = alltweets[-1].id - 1\n",
    "\t\n",
    "\t#keep grabbing tweets until there are no more tweets to grab or the list reaches 2000, otherwise you get RateLimitError\n",
    "\twhile len(new_tweets) > 0 and len(alltweets) < 2000:\n",
    "\t\tprint \"getting tweets before %s\" % (oldest)\n",
    "\t\t\n",
    "\t\t#all subsiquent requests use the max_id param to prevent duplicates\n",
    "\t\tnew_tweets = api.search(q = query ,count=200,max_id=oldest, result_type = 'recent', include_entities = True)\n",
    "\t\t\n",
    "\t\t#save most recent tweets\n",
    "\t\talltweets.extend(new_tweets)\n",
    "\t\t\n",
    "\t\t#update the id of the oldest tweet less one\n",
    "\t\toldest = alltweets[-1].id - 1\n",
    "\t\t\n",
    "\t\tprint \"...%s tweets downloaded so far\" % (len(alltweets))   \n",
    "\t\n",
    "\t#transform the tweepy tweets into a 2D array that will populate the csv\t\n",
    "\touttweets = [[tweet.id_str, query, str(tweet.user.name.encode(\"utf-8\")), tweet.created_at, str(tweet.text.encode(\"utf-8\")).replace('\\n', ' ').replace('\\r', ''), tweet.favorite_count, tweet.retweet_count, tweet.user.followers_count] for tweet in alltweets]\n",
    "\t\n",
    "\t#write the csv\t\n",
    "# \twith open('/users/hnesman/Downloads/DATA_Science/Tweets_Bin/Tweets_Search_%s.csv' % (date) , 'wb') as f:\n",
    "# \twriter = csv.writer(f,delimiter ='|')\n",
    "\twriter.writerow([\"id\",\"query\",\"Username\",\"created_at\",\"text\",\"favorite_count\", \"retweet_count\",\"user's followers count\"])\n",
    "\twriter.writerows(outtweets)\n",
    "\t\n",
    "\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I perform the search, 'query', 'date' are the arguments. I can put a list of terms in query, but no more than 5\n",
    "# can take per call, otherwise you get Rate Limited\n",
    "query = ['bochini']\n",
    "date = '2017-05-09'\n",
    "\n",
    "for i in query:\n",
    "    print i\n",
    "    f = open('/users/hnesman/Downloads/DATA_Science/Tweets_Bin/%s_Tweets_Search_%s.csv' % (i, date) , 'wb')\n",
    "    writer = csv.writer(f,delimiter ='|')\n",
    "    search_tweets(i,date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I set up my credentials for getting access to Twitter API\n",
    "ckey=\"btyxpVnvWkPG1pmnwPLByzse4\"\n",
    "csecret=\"BxvPvVzpW7HwPwy0tWW4skOwcGfFEZ01CtRVPwSpoyjXUVUVf8\"\n",
    "atoken=\"848659773176385536-KtFLOtwUxxGl20cpcyIG5aFusEaR7iT\"\n",
    "asecret=\"pCEy2mgprgfXEB2o76msPV3uFTmfbvGFRozYbCFj6TcFc\"\n",
    "\n",
    "# Initialise Tweepy\n",
    "auth = tweepy.OAuthHandler(ckey, csecret)\n",
    "auth.set_access_token(atoken, asecret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I define 'search_tweets'\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "\n",
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "import csv\n",
    "\n",
    "# Date as YYYY-MM-DD\n",
    "def search_tweets(query,date):\n",
    "\t#Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "\t\n",
    "\t#authorize twitter, initialize tweepy\n",
    "\tauth = tweepy.OAuthHandler(ckey, csecret)\n",
    "\tauth.set_access_token(atoken, asecret)\n",
    "\tapi = tweepy.API(auth)\n",
    "\t\n",
    "\t#initialize a list to hold all the tweepy Tweets\n",
    "\talltweets = []\t\n",
    "\t\n",
    "\t#make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "\tnew_tweets = api.search(q = query,count=10, result_type = 'recent', include_entities =  True)\n",
    "\t\n",
    "\t#save most recent tweets\n",
    "\talltweets.extend(new_tweets)\n",
    "\t\n",
    "\t#save the id of the oldest tweet less one\n",
    "\toldest = alltweets[-1].id - 1\n",
    "\t\n",
    "\t#keep grabbing tweets until there are no tweets left to grab\n",
    "\twhile len(new_tweets) > 0 and len(alltweets) < 2000:\n",
    "\t\tprint \"getting tweets before %s\" % (oldest)\n",
    "\t\t\n",
    "\t\t#all subsiquent requests use the max_id param to prevent duplicates\n",
    "\t\tnew_tweets = api.search(q = query ,count=200,max_id=oldest, result_type = 'recent', include_entities = True)\n",
    "\t\t\n",
    "\t\t#save most recent tweets\n",
    "\t\talltweets.extend(new_tweets)\n",
    "\t\t\n",
    "\t\t#update the id of the oldest tweet less one\n",
    "\t\toldest = alltweets[-1].id - 1\n",
    "\t\t\n",
    "\t\tprint \"...%s tweets downloaded so far\" % (len(alltweets))   \n",
    "\t\n",
    "\t#transform the tweepy tweets into a 2D array that will populate the csv\t\n",
    "\touttweets = [[tweet.id_str, query, str(tweet.user.name.encode(\"utf-8\")), tweet.created_at, str(tweet.text.encode(\"utf-8\")).replace('\\n', ' ').replace('\\r', ''), tweet.favorite_count, tweet.retweet_count, tweet.user.followers_count] for tweet in alltweets]\n",
    "\t\n",
    "\t#write the csv\t\n",
    "# \twith open('/users/hnesman/Downloads/DATA_Science/Tweets_Bin/Tweets_Search_%s.csv' % (date) , 'wb') as f:\n",
    "# \twriter = csv.writer(f,delimiter ='|')\n",
    "\twriter.writerow([\"id\",\"query\",\"Username\",\"created_at\",\"text\",\"favorite_count\", \"retweet_count\",\"user's followers count\"])\n",
    "\twriter.writerows(outtweets)\n",
    "\t\n",
    "\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = ['jefe policía ciudad', 'video analista político']\n",
    "date = '2017-05-06'\n",
    "\n",
    "for i in query:\n",
    "    print i\n",
    "    f = open('/users/hnesman/Downloads/DATA_Science/Tweets_Bin/%s_Tweets_Search_%s.csv' % (i, date) , 'wb')\n",
    "    writer = csv.writer(f,delimiter ='|')\n",
    "    search_tweets(i,date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I saved all the results from the calls in .csv files in a specific folder. The next step is to put together all the files in one dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Este archivo usa el encoding: utf-8\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up all the search Tweets into one file:\n",
    "\n",
    "1) Concatenate the files into a pd.DataFrame\n",
    "\n",
    "2) Save it into a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list with the names of all the files where I have all the Twitter Search Files\n",
    "mypath = '/users/hnesman/Downloads/DATA_Science/Tweets_Bin'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "type(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I delete a fake name that is generated by the loop above\n",
    "del onlyfiles[0]\n",
    "\n",
    "print onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd /users/hnesman/Downloads/DATA_Science/Tweets_Bin\n",
    "\n",
    "# Defines a 'for-loop' for sequencial reading and concatenating all the files in the folder\n",
    "df = pd.concat((pd.read_csv(i,delimiter = '|') for i in onlyfiles))\n",
    "\n",
    "# I drop a column that is created when I concatenated the dataframes\n",
    "df = df.drop([\"id;query;Username;created_at;text;favorite_count;retweet_count;user's followers count\"], axis = 1)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identify which tweets are retweets\n",
    "\n",
    "df['Is RT?'] = df['text'].map(lambda x: 1 if 'RT' in str(x) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I clean the url's\n",
    "\n",
    "df['text'] = df['text'].replace('((www\\.[^\\s]+)|(https?://[^\\s]+))','', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I clean the RT's from the text\n",
    "\n",
    "df['text'] = df['text'].replace('RT','', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check the amount of Tweets in my dataset\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I delete the duplicate tweets\n",
    "\n",
    "df_proc = df.drop_duplicates('id')\n",
    "print len(df_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I drop all the rows with any 'Nan' values and reindex the values\n",
    "\n",
    "df_proc = df_proc.dropna(how='any')\n",
    "df_proc.reset_index(inplace=True)\n",
    "print len(df_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop a column 'index' created\n",
    "df_proc = df_proc.drop(\"index\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed that the results of the Search from Twitter includes tweets not only in Spanish, there are a few tweets in different languages (i.e.: Italian, Portuguese, Japanese!)\n",
    "\n",
    "One of the API Search parameters is 'Lang' which allows select the language of the , but I didn't bother because even in Twitter's Dev site they recommend to use a Language detector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I will use Langid and Langdetect for filtering only tweets in Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Este archivo usa el encoding: utf-8\n",
    "import langid  \n",
    "from langdetect import detect  \n",
    "# import textblob\n",
    "\n",
    "def langid_safe(tweet):  \n",
    "    try:\n",
    "        return langid.classify(tweet)[0]\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "def langdetect_safe(tweet):  \n",
    "    try:\n",
    "        return detect(tweet)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "df_proc['lang_langid'] = df_proc.text.apply(langid_safe)  \n",
    "df_proc['lang_langdetect'] = df_proc.text.apply(langdetect_safe)  \n",
    "\n",
    "# tweets.to_csv('tweets_parsed2.csv', encoding='utf-8')\n",
    "\n",
    "df_proc = df_proc.query(\"lang_langdetect == 'es' or lang_langid == 'es' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now I have narrowed my dataset to Spanish Tweets, the following step is to filter out the Retweets as they don't provide further information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I drop the retweets\n",
    "\n",
    "df_clean = df_proc[df_proc['Is RT?']==0]\n",
    "\n",
    "# And reset the index to keep tidy\n",
    "df_clean.reset_index(inplace=True)\n",
    "len(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got 8663 tweets left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I delete the column 'Index' created when I do 'reset_index'\n",
    "df_clean = df_clean.drop(\"index\", axis = 1)\n",
    "df_clean.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I save my file into .csv, for then being fit it into the Model\n",
    "df_clean.to_csv('/users/hnesman/Downloads/DATA_Science/Argentina_TWEETS.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I will label the tweets, the values I use are P: positive, N: negative, Neu: neutral/no clear sentiment\n",
    "\n",
    "I used Excel for this task, I manually labeled the tweets, saved it in a different file, because I had troubles opening the Tweets File with the 'polarity' column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I open the file with the polarity of the tweets\n",
    "\n",
    "polarity = pd.read_csv('/users/hnesman/Downloads/DATA_Science/Index_polarity.csv', sep = ';',header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Misteriously a couple of tweets were missed during the export from dataframe to csv. It's something to bear in mind when exports to Excel are involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up the name of the columns in the Polarity dataset \n",
    "\n",
    "polarity.columns = ['index','cPolarity']\n",
    "polarity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merging both datasets (there has to be a column in common)\n",
    "\n",
    "df_labels = pd.merge(df_clean, polarity, on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the names of the columns of the new dataframe\n",
    "\n",
    "df_labels.columns = ['index','Username','created_at','favorite_count','id','q','retweet_count','text','user followers count'\n",
    "                    ,'is RT?', 'lang_langid','lang_langdetect','cPolarity']\n",
    "df_labels.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I have my Tweets dataset with polarity, I will clean some information that is irrelevant (to my understanding), and will map a category to each Tweet\n",
    "\n",
    "1) Delete all the Tweets with 'query' = 'sugita' (it is an irrelevant result that came from the search on Media, I shouldn't have considered initially but I didn't know at the time I put together the dataset)\n",
    "\n",
    "2) I will map a category to each query, this might be usefull for Visualisation of the Arg Tweets Dataset.\n",
    "\n",
    "3) I will drop the tweets with 'Polarity' = ['Nan','Neu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete Tweets with query = 'sugita'\n",
    "\n",
    "tweets_arg = df_labels[df_labels.q != 'sugita']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map categories for the 'Search Terms' and create a feature with its value\n",
    "\n",
    "cmap = {'alicia kirchner' : 'politics','vaticano' : 'religion/politics','2x1' : 'politics','macri' : 'politics','fernando cartasegna' : 'politics','afa' : 'sports/politics','macri trump' : 'politics/world','delitos lesa humanidad' : 'politics','\"corte suprema\"' : 'politics','CFK' : 'politics','\"dt de boca\"' : 'sports','grupo macri' : 'politics','planes sociales' : 'politics','maqueda' : 'politics','francisco' : 'religion/politics','lilian tintori' : 'politics/world','Barros Schelotto' : 'sports','river' : 'sports','\"plaza de mayo\"' : 'politics','kirchnerismo' : 'politics'}\n",
    "tweets_arg['category'] = tweets_arg.q.map(lambda x: cmap[str(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map the values of Polarity, in order to filter and manipulate the Tweets based in this feature\n",
    "\n",
    "cmap = {'P': 1,'nan': 2,'Neu': 2, 'N': 0, '0':0, 'NEu': 2,}\n",
    "tweets_arg['Polarity'] = tweets_arg.cPolarity.map(lambda x: cmap[str(x)])\n",
    "tweets_arg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I keep the tweets with Polarity either Positive or Negative\n",
    "\n",
    "tweets_arg_2 = tweets_arg[tweets_arg.Polarity != 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Twitter Dataset\n",
    "\n",
    "### I will narrow my analysis to the tweets that have been labeled with a positive/negative sentiment\n",
    "\n",
    "\n",
    "#### As the plot provides a measure of positivity ratio: Positive tweets/ Total Tweets, sorting by Polarity for each 'q' term we get the ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First I get an overview of which are the sentiments regarding \"Category\"\n",
    "\n",
    "p = sb.factorplot(x='category', \n",
    "              y='Polarity',\n",
    "              kind='bar',\n",
    "              data = tweets_arg_2,\n",
    "              size = 5,\n",
    "            )\n",
    "p.set_xticklabels(rotation=45, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I will take it to a more granular level using \"q\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = sb.factorplot(x='q', \n",
    "              y='Polarity',\n",
    "              kind='bar',\n",
    "              data = tweets_arg_2,\n",
    "              size = 6\n",
    "            )\n",
    "p.set_xticklabels(rotation=45, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## I want to see what are the terms that generate the most positive and most negative feelings\n",
    "\n",
    "- I will sort from higher to lower, to see the most positive\n",
    "\n",
    "- I will sort from lower to higher, to see the most negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I create the dataframes for the most positive terms\n",
    "\n",
    "# I get the top ten results for 'q'\n",
    "query_most = tweets_arg_2.groupby('q').sum().sort_values(by='Polarity', ascending= False)\n",
    "query_top_ten = query_most.iloc[:10]\n",
    "(query_top_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I do the plot for the Top Ten\n",
    "\n",
    "p = sb.factorplot(x='q', \n",
    "              y='Polarity',\n",
    "              kind='bar',\n",
    "              data = query_top_ten,\n",
    "              size = 6\n",
    "            )\n",
    "p.set_xticklabels(rotation=45, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I get the last ten results for 'q'\n",
    "query_less = tweets_arg_2.groupby('q').sum().sort_values(by='Polarity', ascending= True)\n",
    "query_last_ten = query_less.iloc[:10]\n",
    "(query_last_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I do the plot for the Last Ten\n",
    "\n",
    "\n",
    "p = sb.factorplot(x='q', \n",
    "              y='Polarity',\n",
    "              kind='bar',\n",
    "              data = query_last_ten,\n",
    "              size = 6\n",
    "            )\n",
    "p.set_xticklabels(rotation=45, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I want to know which are the top 5 tweets in terms of popularity (measured by Favorite's Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I sort the dataframe by 'Favorite Count' and keep the top 5\n",
    "\n",
    "favorite_tweets = tweets_arg_2.sort_values(by='favorite_count', ascending =  False).iloc[:5][['Username','user followers count','favorite_count','text']]\n",
    "favorite_tweets.to_csv('/users/hnesman/downloads/DATA_Science/favoritetweets.csv')\n",
    "favorite_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcloud\n",
    "\n",
    "### To get another type of glance to the data we can use this type of charts, I will split the dataframe accordingly to its positive or negative feelings\n",
    "\n",
    "1) Split the dataset between positive and negative\n",
    "\n",
    "2) perform Wordclouds for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I separate the positive and negative Tweets\n",
    "\n",
    "positive = tweets_arg_2[tweets_arg_2.Polarity == 1]\n",
    "print len(positive)\n",
    "negative = tweets_arg_2[tweets_arg_2.Polarity == 0]\n",
    "print len(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I will replace a few words in the Datasets that don't apport any information ('que', 'la','lo','de','el')\n",
    "\n",
    "positive['text'] = positive['text'].replace('que | la | el | lo | de','', regex=True)       \n",
    "negative['text'] = negative['text'].replace('que | la | el | lo | de','', regex=True)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Positive Wordcloud\n",
    "wordcloud = WordCloud(width = 1000, height = 500).generate(' '.join(positive1['text']))\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Negative Wordcloud\n",
    "wordcloud = WordCloud(width = 1000, height = 500).generate(' '.join(negative['text']))\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I save my file into .csv, for then being fit it into the Model\n",
    "# df_clean.to_csv('/users/hnesman/Downloads/DATA_Science/Argentina_TWEETS_cuidado.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the last part of the project, perform the Sentiment Analysis\n",
    "\n",
    "### To find the model I will use the TASS dataset, created by the SEPLN (Sociedad Española para el Procesamiento del Lenguaje Natural)\n",
    "\n",
    "Is composed by 3 files of labelled Tweets in Spanish\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the .xml files, get the data I need, save it into a tuple, then save it into a .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I open the file 'General Tweet Train Tagged.csv'\n",
    "try:\n",
    "    general_tweets_corpus_train = pd.read_csv('/users/hnesman/Downloads/DATA_Science/TASS/general-tweets-train-tagged.csv', encoding='utf-8')\n",
    "except:\n",
    "\n",
    "    from lxml import objectify\n",
    "    xml = objectify.parse(open('/users/hnesman/Downloads/DATA_Science/TASS/general-tweets-train-tagged.xml'))\n",
    "    #sample tweet object\n",
    "    root = xml.getroot()\n",
    "    general_tweets_corpus_train = pd.DataFrame(columns=('content', 'polarity', 'agreement'))\n",
    "    tweets = root.getchildren()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content', 'polarity', 'agreement'], [tweet.content.text, tweet.sentiments.polarity.value.text, tweet.sentiments.polarity.type.text]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        general_tweets_corpus_train = general_tweets_corpus_train.append(row_s)\n",
    "    general_tweets_corpus_train.to_csv('/users/hnesman/Downloads/DATA_Science/TASS_Parsed/general-tweets-train-tagged.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "len(general_tweets_corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I open the file 'Stompol Tweets Train Tagged'\n",
    "try:\n",
    "    stompol_tweets_corpus_train = pd.read_csv('/users/hnesman/Downloads/DATA_Science/TASS/stompol-tweets-train-tagged.csv', encoding='utf-8')\n",
    "except:\n",
    "\n",
    "    from lxml import objectify\n",
    "    xml = objectify.parse(open('/users/hnesman/Downloads/DATA_Science/TASS/stompol-tweets-train-tagged.xml'))\n",
    "    #sample tweet object\n",
    "    root = xml.getroot()\n",
    "    stompol_tweets_corpus_train = pd.DataFrame(columns=('content', 'polarity'))\n",
    "    tweets = root.getchildren()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content', 'polarity', 'agreement'], [' '.join(list(tweet.itertext())), tweet.sentiment.get('polarity')]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        stompol_tweets_corpus_train = stompol_tweets_corpus_train.append(row_s)\n",
    "    stompol_tweets_corpus_train.to_csv('/users/hnesman/Downloads/DATA_Science/TASS_Parsed/stompol-tweets-train-tagged.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(stompol_tweets_corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I open the file 'Politics 2013 Tweets Test Tagged'\n",
    "try:\n",
    "    general_tweets_corpus_train = pd.read_csv('/users/hnesman/Downloads/DATA_Science/TASS/politics2013-tweets-test-tagged.csv', encoding='utf-8')\n",
    "except:\n",
    "\n",
    "    from lxml import objectify\n",
    "    xml = objectify.parse(open('/users/hnesman/Downloads/DATA_Science/TASS/politics2013-tweets-test-tagged.xml'))\n",
    "    #sample tweet object\n",
    "    root = xml.getroot()\n",
    "    pol_2013_tweets_corpus_train= pd.DataFrame(columns=('content', 'polarity', 'agreement'))\n",
    "    tweets = root.getchildren()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content', 'polarity', 'agreement'], [tweet.content.text, tweet.sentiments.polarity.value.text, tweet.sentiments.polarity.type.text]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        pol_2013_tweets_corpus_train = pol_2013_tweets_corpus_train.append(row_s)\n",
    "    pol_2013_tweets_corpus_train.to_csv('/users/hnesman/Downloads/DATA_Science/TASS_Parsed/politics2013-tweets-test-tagged.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(pol_2013_tweets_corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I concatenate the 3 dataFrames into the dataframe 'tweets_corpus'\n",
    "\n",
    "tweets_corpus = pd.concat([\n",
    "        pol_2013_tweets_corpus_train,\n",
    "        stompol_tweets_corpus_train,\n",
    "        general_tweets_corpus_train\n",
    "        \n",
    "    ])\n",
    "len(tweets_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building the Model\n",
    "\n",
    "## Set up the labels of the variable 'Polarity', as I want to predict binary values, I have to transform some values of this variable\n",
    "\n",
    "    1) Drop all the rows with polarity value = 'NEU' and 'NONE'\n",
    "    2) Join N and N+, into a new value = 0\n",
    "    3) Join P and P+, into a new value = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I do an inspection of the values in the variable 'polarity'\n",
    "\n",
    "print tweets_corpus.groupby('polarity').count()\n",
    "tweets = tweets_corpus.copy()\n",
    "print tweets.columns\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a cmap for the values of 'polarity'\n",
    "\n",
    "cmap = {'P+': 1, 'P': 1,'NONE': 2,'NEU': 2, 'N': 0, 'N+': 0}\n",
    "tweets['cpolarity'] = tweets.polarity.apply(lambda x: cmap[str(x)])\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check everything run alright\n",
    "print tweets.groupby('cpolarity').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop all the tweets with 'cpolarity' = 2 (accordingly to the map are the 'None' or 'Neu' values)\n",
    "data = tweets[tweets.cpolarity != 2]\n",
    "print data.groupby('cpolarity').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('/users/hnesman/downloads/DATA_Science/TASS.csv', index = False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The lenght of the set and the distribution of values: 1 - Positive, 0 - Negative\n",
    "print len(data)\n",
    "data.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Now my dataset is ready to train a model for Classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Find a model capable to predict Sentiment Analysis on TASS dataset.\n",
    "\n",
    "Models I will try:\n",
    "\n",
    "1) Random Forest Classifier\n",
    "\n",
    "2) Logistic Regression\n",
    "\n",
    "Every Model is built under the frame Grid Search / Cross Validation, to assure the best results:\n",
    "\n",
    "The input of the models are the Word Vectors, and there are a few different ways to approach them:\n",
    "\n",
    "- CountVectorizer (Binary: False/True) / (Freq / Presence)\n",
    "- Tf-idF (I could extract the features ('n-grams') and compare both results\n",
    "\n",
    "I will use both Vectorizing approaches across both models, with the option that provides me best results I will address my dataset (Tweets from Argentina)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## STOP WORDS list\n",
    "\n",
    "I will use the one defined on the Media Research Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I import some libraries I will need\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import grid_search, cross_validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import feature_extraction, ensemble, cross_validation, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### I will use CountVectorizer to Tokenise, filter the 'STOP WORDS' and create the Word Vector:\n",
    "Note:\n",
    "\n",
    "1) The parameter 'max_features' is set to 100.\n",
    "\n",
    "2) The method 'get_feature_names' is useful to detect words that don't add much information, and so, is better to include them in the STOP_WORDS list. I iterate this task until I get satisfactoy results.\n",
    "\n",
    "3) Check that if the parameter 'binary':\n",
    "    a) False, the vector gets the Frequency of the n-gram\n",
    "\n",
    "    b) True, the vector gets the Presence (yes/no) of the n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### I define a function for visualising the performance of the best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Here's a function that plots the ROC curve as well as report model accuracy\n",
    "\n",
    "def plot_bi_roc (X_transformed, y, model, title_text):\n",
    "    y_hat = model.predict(X_transformed)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, y_hat)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label = 'ROC curve (area = %0.2f)' % metrics.auc(fpr, tpr))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([.0, 1.])\n",
    "    plt.ylim([.0, 1.1])\n",
    "    plt.xlabel('FPR/Fall-out')\n",
    "    plt.ylabel('TPR/Sensitivity')\n",
    "    plt.title(title_text+' Sentiment ROC')\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.show()\n",
    "    \n",
    "#END plot_bi_roc ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### I define a function for visualising the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "import itertools\n",
    "# This function plots a confusion matrix \n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cvectorizer_presen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CountVectorizer with Binary = True\n",
    "\n",
    "content = data['content']\n",
    "\n",
    "Cvectorizer_presen = CountVectorizer(max_features = 50, \n",
    "                             ngram_range=(1, 2), \n",
    "                             stop_words= STOP_WORDS,\n",
    "                             binary=True)\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the titles\n",
    "Cvectorizer_presen.fit(content)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "# X_presen = Cvectorizer_presen.transform(content)\n",
    "\n",
    "X_presen = Cvectorizer_presen.transform(content).toarray()\n",
    "y_presen = data['cpolarity']\n",
    "presen = pd.Series(Cvectorizer_presen.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## I split the data set in Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_presen_train,X_presen_test,y_presen_train,y_presen_test=train_test_split(X_presen,y_presen,test_size=0.3)\n",
    "\n",
    "print len(X_presen_train)\n",
    "print len(X_presen_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "Cross Validation enhances the learning capabilities of the model. Cross Validation generates several models in cross sections of the data, measures the performance of each and calculates the mean performance. In this way, we are preventing from overfitting the model.\n",
    "\n",
    "Grid Search optimizes the parameters of the model, providing the best estimator parameters and its scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators = 5)\n",
    "\n",
    "\n",
    "gs_RFC_presen = grid_search.GridSearchCV(estimator = model,\n",
    "                             param_grid = {'n_estimators': [i for i in range(1,52,10)],\n",
    "                                          \"max_depth\": [3, 5],\n",
    "                                          \"bootstrap\": [True, False],\n",
    "                                           \"class_weight\": [None,'balanced'],\n",
    "                                          \"criterion\": [\"gini\"]},\n",
    "                             cv = cross_validation.KFold(n=len(X_presen_train), n_folds=5), scoring='roc_auc')\n",
    "\n",
    "gs_RFC_presen.fit(X_presen_train, y_presen_train)\n",
    "print (gs_RFC_presen.best_estimator_)\n",
    "print (gs_RFC_presen.best_score_)\n",
    "\n",
    "best_RFC_model_presen = gs_RFC_presen.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The score is over 0.5, so we can say that the model is acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Testing the model - ROC curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_bi_roc(X_presen_test, y_presen_test, best_RFC_model_presen, 'RFC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=gs_RFC_presen.predict(X_presen_test)\n",
    "plot_confusion_matrix(confusion_matrix(y_presen_test,y_pred),y_presen_test.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "- I will use the L2 penalisation as L1 is recommended when we have more features than observations. In this case we have 100 features and 7001 observations (tweets)\n",
    "- I will add `random_state` in `param_grid` for Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_LR_presen = grid_search.GridSearchCV(\n",
    "    estimator=LogisticRegression(),\n",
    "    param_grid={'C': [10**-i for i in range(-5, 5)], 'class_weight': [None, 'balanced'],\n",
    "               'random_state':[i for i in range(100,150,10)]},\n",
    "    cv=cross_validation.KFold(n=len(X_presen_train), n_folds=10),\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "gs_LR_presen.fit(X_presen_train, y_presen_train)\n",
    "print (gs_LR_presen.best_estimator_)\n",
    "print (gs_LR_presen.best_score_)\n",
    "\n",
    "best_LR_model_presen = gs_LR_presen.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ROC curve the best estimator´s performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We do the visualisation of the ROC Curve\n",
    "\n",
    "# plot_bi_roc(X_presen_test, y_presen_test, gs_LR_presen, 'Reg Log')\n",
    "plot_bi_roc(X_presen_test, y_presen_test, best_LR_model_presen, 'Reg Log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=gs_LR_presen.predict(X_presen_test)\n",
    "plot_confusion_matrix(confusion_matrix(y_presen_test,y_pred),y_presen_test.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CVectorizer_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CountVectorizer with Binary = False\n",
    "\n",
    "content = data['content']\n",
    "\n",
    "Cvectorizer_freq = CountVectorizer(max_features = 50, \n",
    "                             ngram_range=(1, 2), \n",
    "                             stop_words= STOP_WORDS,\n",
    "                             binary=False)\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the titles\n",
    "Cvectorizer_freq.fit(content)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "# X_freq = Cvectorizer_freq.transform(content)\n",
    "\n",
    "X_freq = Cvectorizer_freq.transform(content).toarray()\n",
    "y_freq = data['cpolarity']\n",
    "\n",
    "freq = pd.Series(Cvectorizer_freq.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_freq_train,X_freq_test,y_freq_train,y_freq_test=train_test_split(X_freq,y_freq,test_size=0.3)\n",
    "\n",
    "print len(X_freq_train)\n",
    "print len(X_freq_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators = 5)\n",
    "\n",
    "\n",
    "gs_RFC_freq = grid_search.GridSearchCV(estimator = model,\n",
    "                             param_grid = {'n_estimators': [i for i in range(1,52,10)],\n",
    "                                          \"max_depth\": [3, 5],\n",
    "                                          \"bootstrap\": [True, False],\n",
    "                                           \"class_weight\": [None,'balanced'],\n",
    "                                          \"criterion\": [\"gini\"]},\n",
    "                             cv = cross_validation.KFold(n=len(X_presen_train), n_folds=5), scoring='roc_auc')\n",
    "\n",
    "gs_RFC_freq.fit(X_freq_train, y_freq_train)\n",
    "print (gs_RFC_freq.best_estimator_)\n",
    "print (gs_RFC_freq.best_score_)\n",
    "\n",
    "best_RFC_model_freq = gs_RFC_freq.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ROC curve the best estimator´s performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_bi_roc(X_freq_test, y_freq_test, best_RFC_model_freq, 'RFC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=gs_RFC_freq.predict(X_freq_test)\n",
    "plot_confusion_matrix(confusion_matrix(y_freq_test,y_pred),y_freq_test.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "- I will use the L2 penalisation as L1 is recommended when we have more features than observations. In this case we have 100 features and 7001 observations (tweets)\n",
    "- I will add `random_state` in `param_grid` for Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_LR_freq = grid_search.GridSearchCV(\n",
    "    estimator=LogisticRegression(),\n",
    "    param_grid={'C': [10**-i for i in range(-5, 5)], 'class_weight': [None, 'balanced'],\n",
    "               'random_state':[i for i in range(100,150,10)]},\n",
    "    cv=cross_validation.KFold(n=len(X_presen_train), n_folds=10),\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "gs_LR_freq.fit(X_freq_train, y_freq_train)\n",
    "print (gs_LR_freq.best_estimator_)\n",
    "print (gs_LR_freq.best_score_)\n",
    "\n",
    "best_LR_model_freq = gs_LR_freq.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve the best estimator´s performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_bi_roc(X_freq_test, y_freq_test, best_LR_model_freq, 'Log Reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=gs_LR_freq.predict(X_freq_test)\n",
    "plot_confusion_matrix(confusion_matrix(y_freq_test,y_pred),y_freq_test.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Tf-idF option for Word Vectorization\n",
    "\n",
    "content = data['content']\n",
    "\n",
    "Tfvectorizer = TfidfVectorizer(max_df=0.5, max_features = 50, ngram_range = (1,2),\n",
    "                                 stop_words=STOP_WORDS)\n",
    "# X_TfidF = Tfvectorizer.fit_transform(content)\n",
    "\n",
    "X_TfidF = Tfvectorizer.fit_transform(content).toarray()\n",
    "TfidF = pd.Series(Tfvectorizer.get_feature_names())\n",
    "\n",
    "y_TfidF = data['cpolarity']\n",
    "\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_TfidF_train,X_TfidF_test,y_TfidF_train,y_TfidF_test=train_test_split(X_TfidF,y_TfidF,test_size=0.3)\n",
    "\n",
    "print len(X_TfidF_train)\n",
    "print len(X_TfidF_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators = 5)\n",
    "\n",
    "\n",
    "gs_RFC_TfidF = grid_search.GridSearchCV(estimator = model,\n",
    "                             param_grid = {'n_estimators': [i for i in range(1,52,10)],\n",
    "                                          \"max_depth\": [3, 5],\n",
    "                                          \"bootstrap\": [True, False],\n",
    "                                           \"class_weight\": [None,'balanced'],\n",
    "                                          \"criterion\": [\"gini\"]},\n",
    "                             cv = cross_validation.KFold(n=len(X_presen_train), n_folds=5), scoring='roc_auc')\n",
    "\n",
    "gs_RFC_TfidF.fit(X_presen_train, y_presen_train)\n",
    "print (gs_RFC_TfidF.best_estimator_)\n",
    "print (gs_RFC_TfidF.best_score_)\n",
    "\n",
    "best_RFC_model_TfidF = gs_RFC_TfidF.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve the best estimator´s performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_bi_roc(X_TfidF_test, y_TfidF_test, best_RFC_model_TfidF, 'RFC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=gs_RFC_TfidF.predict(X_TfidF_test)\n",
    "plot_confusion_matrix(confusion_matrix(y_TfidF_test,y_pred),y_TfidF_test.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "- I will use the L2 penalisation as L1 is recommended when we have more features than observations. In this case we have 100 features and 7001 observations (tweets)\n",
    "- I will add `random_state` in `param_grid` for Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_LR_TfidF = grid_search.GridSearchCV(\n",
    "    estimator=LogisticRegression(),\n",
    "    param_grid={'C': [10**-i for i in range(-5, 5)], 'class_weight': [None, 'balanced'],\n",
    "               'random_state':[i for i in range(100,150,10)]},\n",
    "    cv=cross_validation.KFold(n=len(X_presen_train), n_folds=10),\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "gs_LR_TfidF.fit(X_TfidF_train, y_TfidF_train)\n",
    "print (gs_LR_TfidF.best_estimator_)\n",
    "print (gs_LR_TfidF.best_score_)\n",
    "\n",
    "best_LR_model_TfidF = gs_LR_TfidF.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve the best estimator´s performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_bi_roc(X_TfidF_test, y_TfidF_test, best_LR_model_TfidF, 'Log Reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=gs_LR_TfidF.predict(X_TfidF_test)\n",
    "plot_confusion_matrix(confusion_matrix(y_TfidF_test,y_pred),y_TfidF_test.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now I will perform the Sentiment Analysis over the dataset I collected\n",
    "\n",
    "Log Reg models perform better, so I am choose this one for my dataset. I will try Presen and TfidF for vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I open the file and check that is the correct one\n",
    "#!/usr/bin/env python\n",
    "# Este archivo usa el encoding: utf-8\n",
    "df_label = pd.read_csv('/users/hnesman/Downloads/DATA_Science/Tweets_Arg_Label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop the column 'Unnamed: 0', that is generated when opening the file\n",
    "\n",
    "df_label = df_label.drop([\"Unnamed: 0\",\"index\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer Presence / (Binary = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check there is no null values\n",
    "df_label.text.isnull().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Este archivo usa el encoding: utf-8\n",
    "\n",
    "# content = df_label[('text')].dropna(inplace=True)\n",
    "\n",
    "Cvect_presen_TA = CountVectorizer(max_features = 50, \n",
    "                             ngram_range=(1, 2), \n",
    "                             stop_words= STOP_WORDS,\n",
    "                             binary=True)\n",
    "\n",
    "X_presen_TA = Cvect_presen_TA.fit_transform(df_label.text.str.decode('utf-8', 'ignore')).toarray()\n",
    "y_presen_TA = df_label['Polarity']\n",
    "\n",
    "\n",
    "# TO GET A SERIES WITH THE FEATURES FROM CVECT_PRESEN\n",
    "presen_TA = pd.Series(Cvect_presen_TA.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Split the data (Train, Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_presen_TA_train,X_presen_TA_test,y_presen_TA_train,y_presen_TA_test=train_test_split(X_presen_TA,y_presen_TA,test_size=0.3)\n",
    "\n",
    "print len(X_presen_TA_train)\n",
    "print len(X_presen_TA_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_LR_presen_TA = grid_search.GridSearchCV(\n",
    "    estimator=LogisticRegression(),\n",
    "    param_grid={'C': [10**-i for i in range(-5, 5)], 'class_weight': [None, 'balanced'],\n",
    "               'random_state':[i for i in range(100,150,10)]},\n",
    "    cv=cross_validation.KFold(n=len(X_presen_TA_train), n_folds=10),\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "gs_LR_presen_TA.fit(X_presen_TA_train, y_presen_TA_train)\n",
    "print (gs_LR_presen_TA.best_estimator_)\n",
    "print (gs_LR_presen_TA.best_score_)\n",
    "\n",
    "best_LR_model_presen_TA = gs_LR_presen_TA.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve the best estimator´s performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_bi_roc(X_presen_TA_test, y_presen_TA_test, best_LR_model_presen_TA, 'Log Reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=gs_LR_presen_TA.predict(X_presen_TA_test)\n",
    "plot_confusion_matrix(confusion_matrix(y_presen_TA_test,y_pred),y_presen_TA_test.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Tf-idF option for Word Vectorization\n",
    "\n",
    "# content = data['content']\n",
    "\n",
    "TfidF_TA_vectorizer = TfidfVectorizer(max_df=0.5, max_features = 50, ngram_range = (1,2),\n",
    "                                 stop_words=STOP_WORDS)\n",
    "\n",
    "# X_TfidF = Tfvectorizer.fit_transform(content)\n",
    "\n",
    "# TO CREATE THE WORD VECTOR - TFIDF\n",
    "X_TfidF_TA = TfidF_TA_vectorizer.fit_transform(df_label.text.str.decode('utf-8', 'ignore')).toarray()\n",
    "y_TfidF_TA = df_label['Polarity']\n",
    "\n",
    "# TO GET A SERIES WITH THE FEATURES FROM TFIDF\n",
    "TfidF_TA = pd.Series(TfidF_TA_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_TfidF_TA_train,X_TfidF_TA_test,y_TfidF_TA_train,y_TfidF_TA_test=train_test_split(X_TfidF_TA,y_TfidF_TA,test_size=0.3)\n",
    "\n",
    "print len(X_TfidF_TA_train)\n",
    "print len(X_TfidF_TA_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_LR_TfidF_TA = grid_search.GridSearchCV(\n",
    "    estimator=LogisticRegression(),\n",
    "    param_grid={'C': [10**-i for i in range(-5, 5)], 'class_weight': [None, 'balanced'],\n",
    "               'random_state':[i for i in range(100,150,10)]},\n",
    "    cv=cross_validation.KFold(n=len(X_TfidF_TA_train), n_folds=10),\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "gs_LR_TfidF_TA.fit(X_TfidF_TA_train, y_TfidF_TA_train)\n",
    "\n",
    "print (gs_LR_TfidF_TA.best_estimator_)\n",
    "print (gs_LR_TfidF_TA.best_score_)\n",
    "\n",
    "best_LR_model_TfidF_TA = gs_LR_TfidF_TA.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve the best estimator´s performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_bi_roc(X_TfidF_TA_test, y_TfidF_TA_test, best_LR_model_TfidF_TA, 'Log Reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=gs_LR_TfidF.predict(X_TfidF_test)\n",
    "plot_confusion_matrix(confusion_matrix(y_TfidF_test,y_pred),y_TfidF_test.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
