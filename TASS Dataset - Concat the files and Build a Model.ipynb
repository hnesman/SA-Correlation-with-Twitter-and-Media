{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the .xml files, get the data I need, save it into a tuple, then save it into a .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    general_tweets_corpus_train = pd.read_csv('/users/hnesman/Downloads/DATA_Science/TASS/general-tweets-train-tagged.csv', encoding='utf-8')\n",
    "except:\n",
    "\n",
    "    from lxml import objectify\n",
    "    xml = objectify.parse(open('/users/hnesman/Downloads/DATA_Science/TASS/general-tweets-train-tagged.xml'))\n",
    "    #sample tweet object\n",
    "    root = xml.getroot()\n",
    "    general_tweets_corpus_train = pd.DataFrame(columns=('content', 'polarity', 'agreement'))\n",
    "    tweets = root.getchildren()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content', 'polarity', 'agreement'], [tweet.content.text, tweet.sentiments.polarity.value.text, tweet.sentiments.polarity.type.text]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        general_tweets_corpus_train = general_tweets_corpus_train.append(row_s)\n",
    "    general_tweets_corpus_train.to_csv('/users/hnesman/Downloads/DATA_Science/TASS_Parsed/general-tweets-train-tagged.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7219"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(general_tweets_corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    stompol_tweets_corpus_train = pd.read_csv('/users/hnesman/Downloads/DATA_Science/TASS/stompol-tweets-train-tagged.csv', encoding='utf-8')\n",
    "except:\n",
    "\n",
    "    from lxml import objectify\n",
    "    xml = objectify.parse(open('/users/hnesman/Downloads/DATA_Science/TASS/stompol-tweets-train-tagged.xml'))\n",
    "    #sample tweet object\n",
    "    root = xml.getroot()\n",
    "    stompol_tweets_corpus_train = pd.DataFrame(columns=('content', 'polarity'))\n",
    "    tweets = root.getchildren()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content', 'polarity', 'agreement'], [' '.join(list(tweet.itertext())), tweet.sentiment.get('polarity')]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        stompol_tweets_corpus_train = stompol_tweets_corpus_train.append(row_s)\n",
    "    stompol_tweets_corpus_train.to_csv('/users/hnesman/Downloads/DATA_Science/TASS_Parsed/stompol-tweets-train-tagged.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stompol_tweets_corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hnesman/Downloads/DATA_Science/TASS\n"
     ]
    }
   ],
   "source": [
    "# cd /Users/hnesman/Downloads/DATA_Science/TASS\n",
    "# !head -n 30 general-tweets-train-tagged.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    general_tweets_corpus_train = pd.read_csv('/users/hnesman/Downloads/DATA_Science/TASS/politics2013-tweets-test-tagged.csv', encoding='utf-8')\n",
    "except:\n",
    "\n",
    "    from lxml import objectify\n",
    "    xml = objectify.parse(open('/users/hnesman/Downloads/DATA_Science/TASS/politics2013-tweets-test-tagged.xml'))\n",
    "    #sample tweet object\n",
    "    root = xml.getroot()\n",
    "    pol_2013_tweets_corpus_train= pd.DataFrame(columns=('content', 'polarity', 'agreement'))\n",
    "    tweets = root.getchildren()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content', 'polarity', 'agreement'], [tweet.content.text, tweet.sentiments.polarity.value.text, tweet.sentiments.polarity.type.text]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        pol_2013_tweets_corpus_train = pol_2013_tweets_corpus_train.append(row_s)\n",
    "    pol_2013_tweets_corpus_train.to_csv('/users/hnesman/Downloads/DATA_Science/TASS_Parsed/politics2013-tweets-test-tagged.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pol_2013_tweets_corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agreement</th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>\"@marianorajoy: En España las cosas se pueden, se deben y se van a hacer infinitamente mejor que estos últimos 4 años\" Eso son soluciones!!</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DISAGREEMENT</td>\n",
       "      <td>En PSO€ el que no corre vuela, todavía caliente el cadáver político de ZP y Rubalcaba y la PANtumaca buscando hueco #votaPP #sumatealcambio</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>#nomeolvido de q cuando se aprobo la ley del aborto libre todas tus ministras saltaban de alegria en el congreso y yo lloré @ConRubalcaba</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>#CCOO exige al nuevo Gobierno que reactive el mercado interno de automoción para relanzar al sector #20N : http://t.co/XScRUEKh via @AddThis</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DISAGREEMENT</td>\n",
       "      <td>@marianorajoy A mí inviable me parecen los fraudes fiscales que cometen miembros de su partido no una ley que afecta a miles de familias.</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      agreement  \\\n",
       "0     AGREEMENT   \n",
       "1  DISAGREEMENT   \n",
       "2     AGREEMENT   \n",
       "3     AGREEMENT   \n",
       "4  DISAGREEMENT   \n",
       "\n",
       "                                                                                                                                        content  \\\n",
       "0   \"@marianorajoy: En España las cosas se pueden, se deben y se van a hacer infinitamente mejor que estos últimos 4 años\" Eso son soluciones!!   \n",
       "1   En PSO€ el que no corre vuela, todavía caliente el cadáver político de ZP y Rubalcaba y la PANtumaca buscando hueco #votaPP #sumatealcambio   \n",
       "2     #nomeolvido de q cuando se aprobo la ley del aborto libre todas tus ministras saltaban de alegria en el congreso y yo lloré @ConRubalcaba   \n",
       "3  #CCOO exige al nuevo Gobierno que reactive el mercado interno de automoción para relanzar al sector #20N : http://t.co/XScRUEKh via @AddThis   \n",
       "4     @marianorajoy A mí inviable me parecen los fraudes fiscales que cometen miembros de su partido no una ley que afecta a miles de familias.   \n",
       "\n",
       "  polarity  \n",
       "0        P  \n",
       "1        N  \n",
       "2        N  \n",
       "3      NEU  \n",
       "4        P  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_corpus = pd.concat([\n",
    "        pol_2013_tweets_corpus_train,\n",
    "        stompol_tweets_corpus_train,\n",
    "        general_tweets_corpus_train\n",
    "        \n",
    "    ])\n",
    "tweets_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model\n",
    "\n",
    "## Set up the labels of the variable 'Polarity', as I want to predict binary values, I have to transform some values of this variable\n",
    "\n",
    "    1) I will set up my dataset 'tweets_corpus', I will drop all those rows with polarity = 'NEU' , 'NONE'\n",
    "    2) I will join N with N+, into a new value = 0\n",
    "    3) I will join P with P+, into a new value = 1\n",
    "\n",
    "## Find a model capable to do a good perform with Tweets in Spanish (that's why I use TASS)\n",
    "\n",
    "    1) Random Forest Classificator\n",
    "    2) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          agreement  content\n",
      "polarity                    \n",
      "N              2033     2468\n",
      "N+              847      847\n",
      "NEU            1611     1797\n",
      "NONE           1705     1705\n",
      "P              1871     2034\n",
      "P+             1652     1652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10503"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I do an inspection of the values in the variable 'polarity'\n",
    "\n",
    "print tweets_corpus.groupby('polarity').count()\n",
    "tweets = tweets_corpus.copy()\n",
    "tweets.columns\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agreement</th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "      <th>cpolarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>\"@marianorajoy: En España las cosas se pueden, se deben y se van a hacer infinitamente mejor que estos últimos 4 años\" Eso son soluciones!!</td>\n",
       "      <td>P</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DISAGREEMENT</td>\n",
       "      <td>En PSO€ el que no corre vuela, todavía caliente el cadáver político de ZP y Rubalcaba y la PANtumaca buscando hueco #votaPP #sumatealcambio</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>#nomeolvido de q cuando se aprobo la ley del aborto libre todas tus ministras saltaban de alegria en el congreso y yo lloré @ConRubalcaba</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>#CCOO exige al nuevo Gobierno que reactive el mercado interno de automoción para relanzar al sector #20N : http://t.co/XScRUEKh via @AddThis</td>\n",
       "      <td>NEU</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DISAGREEMENT</td>\n",
       "      <td>@marianorajoy A mí inviable me parecen los fraudes fiscales que cometen miembros de su partido no una ley que afecta a miles de familias.</td>\n",
       "      <td>P</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      agreement  \\\n",
       "0     AGREEMENT   \n",
       "1  DISAGREEMENT   \n",
       "2     AGREEMENT   \n",
       "3     AGREEMENT   \n",
       "4  DISAGREEMENT   \n",
       "\n",
       "                                                                                                                                        content  \\\n",
       "0   \"@marianorajoy: En España las cosas se pueden, se deben y se van a hacer infinitamente mejor que estos últimos 4 años\" Eso son soluciones!!   \n",
       "1   En PSO€ el que no corre vuela, todavía caliente el cadáver político de ZP y Rubalcaba y la PANtumaca buscando hueco #votaPP #sumatealcambio   \n",
       "2     #nomeolvido de q cuando se aprobo la ley del aborto libre todas tus ministras saltaban de alegria en el congreso y yo lloré @ConRubalcaba   \n",
       "3  #CCOO exige al nuevo Gobierno que reactive el mercado interno de automoción para relanzar al sector #20N : http://t.co/XScRUEKh via @AddThis   \n",
       "4     @marianorajoy A mí inviable me parecen los fraudes fiscales que cometen miembros de su partido no una ley que afecta a miles de familias.   \n",
       "\n",
       "  polarity  cpolarity  \n",
       "0        P          1  \n",
       "1        N          0  \n",
       "2        N          0  \n",
       "3      NEU          2  \n",
       "4        P          1  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a cmap for the values of 'polarity'\n",
    "\n",
    "cmap = {'P+': 1, 'P': 1,'NONE': 2,'NEU': 2, 'N': 0, 'N+': 0}\n",
    "tweets['cpolarity'] = tweets.polarity.apply(lambda x: cmap[str(x)])\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           agreement  content  polarity\n",
      "cpolarity                              \n",
      "0               2880     3315      3315\n",
      "1               3523     3686      3686\n",
      "2               3316     3502      3502\n"
     ]
    }
   ],
   "source": [
    "# Check everything run alright\n",
    "print tweets.groupby('cpolarity').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.526496\n",
       "0    0.473504\n",
       "Name: cpolarity, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all the tweets with 'cpolarity' = 2 (accordingly to the map are the 'None' or 'Neu' values)\n",
    "data = tweets[tweets.cpolarity != 2]\n",
    "data.cpolarity.value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7001"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print data.groupby('cpolarity').count()\n",
    "data['cpolarity'].dtype\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now my dataset is ready to train a model for Classification:\n",
    "\n",
    "-I will try a Random Forest to predict the polarity of the Tweet, based on the Features of the Content\n",
    "    - So I need to extract the Features, then fit the model, and see test the results\n",
    "    - For CountVectorizer I am gonna use the same model I used in the MediaNews analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hnesman/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I use the same list of Stop Words that I used for Argentina Tweets\n",
    "\n",
    "# coding: utf8\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "\n",
    "STOP_WORDS = set(\"\"\" 100 20 http https 'http co' 'https co' 'cont co' cont co\n",
    "none anos ano año años actualmente acuerdo adelante ademas además adrede afirmó agregó ahi ahora ahí\n",
    "al algo alguna algunas alguno algunos algún alli allí alrededor ambos ampleamos\n",
    "antano antaño ante anterior antes apenas aproximadamente aquel aquella aquellas\n",
    "aquello aquellos aqui aquél aquélla aquéllas aquéllos aquí arriba arribaabajo\n",
    "aseguró asi así atras aun aunque ayer añadió aún\n",
    "bajo bastante bien breve buen buena buenas bueno buenos\n",
    "cada casi cerca cierta ciertas cierto ciertos cinco claro comentó como con\n",
    "conmigo conocer conseguimos conseguir considera consideró consigo consigue\n",
    "consiguen consigues contigo contra cosas creo cual cuales cualquier cuando\n",
    "cuanta cuantas cuanto cuantos cuatro cuenta cuál cuáles cuándo cuánta cuántas\n",
    "cuánto cuántos cómo\n",
    "da dado dan dar de debajo debe deben debido decir dejó del delante demasiado\n",
    "demás dentro deprisa desde despacio despues después detras detrás dia dias dice\n",
    "dicen dicho dieron diferente diferentes dijeron dijo dio donde dos durante día\n",
    "días dónde\n",
    "ejemplo el ella ellas ello ellos embargo empleais emplean emplear empleas\n",
    "empleo en encima encuentra enfrente enseguida entonces entre era eramos eran\n",
    "eras eres es esa esas ese eso esos esta estaba estaban estado estados estais\n",
    "estamos estan estar estará estas este esto estos estoy estuvo está están ex\n",
    "excepto existe existen explicó expresó él ésa ésas ése ésos ésta éstas éste\n",
    "éstos\n",
    "fin final fue fuera fueron fui fuimos\n",
    "general gran grandes gueno\n",
    "ha haber habia habla hablan habló habrá había habían hace haceis hacemos hacen hacer\n",
    "hacerlo haces hacia haciendo hago han hasta hay haya he hecho hemos hicieron\n",
    "hizo horas hoy hubo\n",
    "igual incluso indicó informo informó intenta intentais intentamos intentan\n",
    "intentar intentas intento ir\n",
    "junto\n",
    "la lado largo las le lejos les llegó lleva llevar lo los luego lugar\n",
    "mal manera manifestó mas mayor me mediante medio mejor mencionó menos menudo mi\n",
    "mia mias mientras mio mios mis misma mismas mismo mismos modo momento mucha\n",
    "muchas mucho muchos muy más mí mía mías mío míos\n",
    "nada nadie ni ninguna ningunas ninguno ningunos ningún no nos nosotras nosotros\n",
    "nuestra nuestras nuestro nuestros nueva nuevas nuevo nuevos nunca\n",
    "ocho os otra otras otro otros\n",
    "pais para parece parte partir pasada pasado paìs peor pero pesar poca pocas\n",
    "poco pocos podeis podemos poder podria podriais podriamos podrian podrias podrá\n",
    "podrán podría podrían poner por porque posible primer primera primero primeros\n",
    "principalmente pronto propia propias propio propios proximo próximo próximos\n",
    "pudo pueda puede pueden puedo pues\n",
    "qeu que quedó queremos quien quienes quiere quiza quizas quizá quizás quién quiénes qué\n",
    "raras realizado realizar realizó repente respecto\n",
    "sabe sabeis sabemos saben saber sabes salvo se sea sean segun segunda segundo\n",
    "según seis ser sera será serán sería señaló si sido siempre siendo siete sigue\n",
    "siguiente sin sino sobre sois sola solamente solas solo solos somos son soy\n",
    "soyos su supuesto sus suya suyas suyo sé sí sólo\n",
    "tal tambien también tampoco tan tanto tarde te temprano tendrá tendrán teneis\n",
    "tenemos tener tenga tengo tenido tenía tercera ti tiempo tiene tienen toda\n",
    "todas todavia todavía todo todos total trabaja trabajais trabajamos trabajan\n",
    "trabajar trabajas trabajo tras trata través tres tu tus tuvo tuya tuyas tuyo\n",
    "tuyos tú\n",
    "ultimo un una unas uno unos usa usais usamos usan usar usas uso usted ustedes\n",
    "última últimas último últimos\n",
    "va vais valor vamos van varias varios vaya veces ver verdad verdadera verdadero\n",
    "vez vosotras vosotros voy vuestra vuestras vuestro vuestros\n",
    "ya yo\n",
    "\"\"\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'agreement', u'content', u'polarity', u'cpolarity'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'000',\n",
       " u'20n',\n",
       " u'30',\n",
       " u'abrazo',\n",
       " u'agarzon',\n",
       " u'ahorapodemos',\n",
       " u'albert_rivera',\n",
       " u'alejandrosanz',\n",
       " u'amigos',\n",
       " u'andaluc\\xeda',\n",
       " u'apoyo',\n",
       " u'cadavotovale',\n",
       " u'cambio',\n",
       " u'campa\\xf1a',\n",
       " u'casa',\n",
       " u'cayo_lara',\n",
       " u'chac\\xf3n',\n",
       " u'ciudadanoscs',\n",
       " u'congreso',\n",
       " u'conrubalcaba',\n",
       " u'crisis',\n",
       " u'david_busta',\n",
       " u'democracia',\n",
       " u'derechos',\n",
       " u'deuda',\n",
       " u'dinero',\n",
       " u'dl',\n",
       " u'domingo',\n",
       " u'd\\xe9ficit',\n",
       " u'educaci\\xf3n',\n",
       " u'elecciones',\n",
       " u'electoral',\n",
       " u'enhorabuena',\n",
       " u'equipo',\n",
       " u'espa\\xf1a',\n",
       " u'espa\\xf1oles',\n",
       " u'espero',\n",
       " u'eta',\n",
       " u'euros',\n",
       " u'falta',\n",
       " u'felicidades',\n",
       " u'feliz',\n",
       " u'ff',\n",
       " u'foto',\n",
       " u'gente',\n",
       " u'gobierno',\n",
       " u'gracias',\n",
       " u'gusta',\n",
       " u'huelga',\n",
       " u'impuestos',\n",
       " u'iu',\n",
       " u'iunida',\n",
       " u'javierarenas_pp',\n",
       " u'junta',\n",
       " u'justicia',\n",
       " u'laboral',\n",
       " u'ley',\n",
       " u'madrid',\n",
       " u'marianorajoy',\n",
       " u'mariviromero',\n",
       " u'ma\\xf1ana',\n",
       " u'millones',\n",
       " u'ministro',\n",
       " u'mundo',\n",
       " u'noche',\n",
       " u'noches',\n",
       " u'nomeolvido',\n",
       " u'paro',\n",
       " u'partido',\n",
       " u'pa\\xeds',\n",
       " u'pedroj_ramirez',\n",
       " u'personas',\n",
       " u'pol\\xedtica',\n",
       " u'pp',\n",
       " u'ppopular',\n",
       " u'presidente',\n",
       " u'programa',\n",
       " u'psoe',\n",
       " u'p\\xfablico',\n",
       " u'quiero',\n",
       " u'rajoy',\n",
       " u'recortes',\n",
       " u'reforma',\n",
       " u'rt',\n",
       " u'rubalcaba',\n",
       " u'sanchezcastejon',\n",
       " u'sanidad',\n",
       " u'semana',\n",
       " u'sevilla',\n",
       " u'sumatealcambio',\n",
       " u'telediariointer',\n",
       " u'telediariointer 30',\n",
       " u'upyd',\n",
       " u'valencia',\n",
       " u'via',\n",
       " u'vida',\n",
       " u'votapp',\n",
       " u'votar',\n",
       " u'voto',\n",
       " u'v\\xeda']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = data['content']\n",
    "Cvectorizer = CountVectorizer(max_features = 100, \n",
    "                             ngram_range=(1, 2), \n",
    "                             stop_words= STOP_WORDS,\n",
    "                             binary=False)\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the titles\n",
    "Cvectorizer.fit(content)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = Cvectorizer.transform(content)\n",
    "Cvectorizer.get_feature_names()\n",
    "# X.shape\n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use a Tf-idF for Word Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Tfvectorizer = TfidfVectorizer(max_df=0.5, max_features = 100,\n",
    "                                 stop_words=STOP_WORDS)\n",
    "X_TfidF = Tfvectorizer.fit_transform(content)\n",
    "Tfvectorizer.get_feature_names()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using 'vectorizer.get_feature_names' to adjust the STOP WORDS, I took off http https co cont\n",
    "# feat = vectorizer.get_feature_names()\n",
    "# feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "CV AUC [ 0.68500602  0.70804281  0.68439835], Average AUC 0.692482394466\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 25)\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the titles\n",
    "vectorizer.fit(content)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(content).toarray()\n",
    "y = data['cpolarity']\n",
    "\n",
    "\n",
    "print type(X)\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc')\n",
    "print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Importance Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>gracias</td>\n",
       "      <td>0.065722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>ppopular</td>\n",
       "      <td>0.022888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>psoe</td>\n",
       "      <td>0.020824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>iunida</td>\n",
       "      <td>0.020582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>pp</td>\n",
       "      <td>0.020259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Features  Importance Score\n",
       "46   gracias          0.065722\n",
       "74  ppopular          0.022888\n",
       "77      psoe          0.020824\n",
       "51    iunida          0.020582\n",
       "73        pp          0.020259"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)\n",
    "\n",
    "all_feature_names = vectorizer.get_feature_names()\n",
    "feature_importances = pd.DataFrame({'Features' : all_feature_names, 'Importance Score': model.feature_importances_})\n",
    "feature_importances.sort_values('Importance Score', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.52187, std: 0.01844, params: {u'n_estimators': 1, u'bootstrap': True, u'criterion': u'gini', u'max_depth': 3}, mean: 0.60861, std: 0.02824, params: {u'n_estimators': 11, u'bootstrap': True, u'criterion': u'gini', u'max_depth': 3}, mean: 0.63720, std: 0.04157, params: {u'n_estimators': 21, u'bootstrap': True, u'criterion': u'gini', u'max_depth': 3}, mean: 0.65228, std: 0.03877, params: {u'n_estimators': 31, u'bootstrap': True, u'criterion': u'gini', u'max_depth': 3}, mean: 0.65206, std: 0.03929, params: {u'n_estimators': 41, u'bootstrap': True, u'criterion': u'gini', u'max_depth': 3}, mean: 0.65398, std: 0.03606, params: {u'n_estimators': 51, u'bootstrap': True, u'criterion': u'gini', u'max_depth': 3}, mean: 0.55220, std: 0.02513, params: {u'n_estimators': 1, u'bootstrap': True, u'criterion': u'gini', u'max_depth': 5}, mean: 0.63408, std: 0.03896, params: {u'n_estimators': 11, u'bootstrap': True, u'criterion': u'gini', u'max_depth': 5}, mean: 0.65732, std: 0.04484, params: {u'n_estimators': 21, u'bootstrap': True, u'criterion': u'gini', u'max_depth': 5}, mean: 0.65897, std: 0.04294, params: {u'n_estimators': 31, u'bootstrap': True, u'criterion': u'gini', u'max_depth': 5}, mean: 0.66576, std: 0.04077, params: {u'n_estimators': 41, u'bootstrap': True, u'criterion': u'gini', u'max_depth': 5}, mean: 0.66563, std: 0.04089, params: {u'n_estimators': 51, u'bootstrap': True, u'criterion': u'gini', u'max_depth': 5}, mean: 0.53905, std: 0.01670, params: {u'n_estimators': 1, u'bootstrap': False, u'criterion': u'gini', u'max_depth': 3}, mean: 0.62383, std: 0.03517, params: {u'n_estimators': 11, u'bootstrap': False, u'criterion': u'gini', u'max_depth': 3}, mean: 0.63064, std: 0.04178, params: {u'n_estimators': 21, u'bootstrap': False, u'criterion': u'gini', u'max_depth': 3}, mean: 0.64240, std: 0.03947, params: {u'n_estimators': 31, u'bootstrap': False, u'criterion': u'gini', u'max_depth': 3}, mean: 0.63960, std: 0.03914, params: {u'n_estimators': 41, u'bootstrap': False, u'criterion': u'gini', u'max_depth': 3}, mean: 0.64979, std: 0.03684, params: {u'n_estimators': 51, u'bootstrap': False, u'criterion': u'gini', u'max_depth': 3}, mean: 0.54268, std: 0.03283, params: {u'n_estimators': 1, u'bootstrap': False, u'criterion': u'gini', u'max_depth': 5}, mean: 0.63069, std: 0.03384, params: {u'n_estimators': 11, u'bootstrap': False, u'criterion': u'gini', u'max_depth': 5}, mean: 0.64378, std: 0.04457, params: {u'n_estimators': 21, u'bootstrap': False, u'criterion': u'gini', u'max_depth': 5}, mean: 0.65011, std: 0.04137, params: {u'n_estimators': 31, u'bootstrap': False, u'criterion': u'gini', u'max_depth': 5}, mean: 0.65621, std: 0.03910, params: {u'n_estimators': 41, u'bootstrap': False, u'criterion': u'gini', u'max_depth': 5}, mean: 0.65458, std: 0.03851, params: {u'n_estimators': 51, u'bootstrap': False, u'criterion': u'gini', u'max_depth': 5}]\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion=u'gini',\n",
      "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=41, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "0.665764361852\n"
     ]
    }
   ],
   "source": [
    "from sklearn import grid_search, cross_validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "### I defined this above, this only for reference ###\n",
    "# vectorizer.fit(content)\n",
    "# X = vectorizer.transform(content).toarray()\n",
    "# y = data['cpolarity']\n",
    "\n",
    "# scores = cross_val_score(model, X, y, scoring='roc_auc') # tengo que ver cómo meto las Cross Validation dentro de GS\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 5)\n",
    "\n",
    "kf = KFold(n_splits=2)\n",
    "\n",
    "gs = grid_search.GridSearchCV(estimator = model,\n",
    "                             param_grid = {'n_estimators': [i for i in range(1,52,10)],\n",
    "                                          \"max_depth\": [3, 5],\n",
    "                                          \"bootstrap\": [True, False],\n",
    "                                          \"criterion\": [\"gini\"]},\n",
    "                             cv = cross_validation.KFold(n=len(X), n_folds=10), scoring='roc_auc')\n",
    "\n",
    "gs.fit(X, y)\n",
    "print gs.grid_scores_\n",
    "print (gs.best_estimator_)\n",
    "print (gs.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now I will try a Logistic Regression fit of my data, I will use Grid Search as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.70697, std: 0.04129, params: {u'C': 100000, u'class_weight': None}, mean: 0.70693, std: 0.04172, params: {u'C': 100000, u'class_weight': u'balanced'}, mean: 0.70697, std: 0.04129, params: {u'C': 10000, u'class_weight': None}, mean: 0.70693, std: 0.04172, params: {u'C': 10000, u'class_weight': u'balanced'}, mean: 0.70699, std: 0.04130, params: {u'C': 1000, u'class_weight': None}, mean: 0.70696, std: 0.04169, params: {u'C': 1000, u'class_weight': u'balanced'}, mean: 0.70712, std: 0.04103, params: {u'C': 100, u'class_weight': None}, mean: 0.70712, std: 0.04146, params: {u'C': 100, u'class_weight': u'balanced'}, mean: 0.70774, std: 0.03944, params: {u'C': 10, u'class_weight': None}, mean: 0.70766, std: 0.03995, params: {u'C': 10, u'class_weight': u'balanced'}, mean: 0.71009, std: 0.03673, params: {u'C': 1, u'class_weight': None}, mean: 0.71033, std: 0.03641, params: {u'C': 1, u'class_weight': u'balanced'}, mean: 0.70622, std: 0.03602, params: {u'C': 0.1, u'class_weight': None}, mean: 0.70619, std: 0.03615, params: {u'C': 0.1, u'class_weight': u'balanced'}, mean: 0.69326, std: 0.03682, params: {u'C': 0.01, u'class_weight': None}, mean: 0.69313, std: 0.03686, params: {u'C': 0.01, u'class_weight': u'balanced'}, mean: 0.68855, std: 0.03675, params: {u'C': 0.001, u'class_weight': None}, mean: 0.68814, std: 0.03684, params: {u'C': 0.001, u'class_weight': u'balanced'}, mean: 0.68767, std: 0.03540, params: {u'C': 0.0001, u'class_weight': None}, mean: 0.68725, std: 0.03705, params: {u'C': 0.0001, u'class_weight': u'balanced'}]\n",
      "LogisticRegression(C=1, class_weight=u'balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "0.710333867807\n"
     ]
    }
   ],
   "source": [
    "gs = grid_search.GridSearchCV(\n",
    "    estimator=LogisticRegression(),\n",
    "    param_grid={'C': [10**-i for i in range(-5, 5)], 'class_weight': [None, 'balanced']},\n",
    "    cv=cross_validation.KFold(n=len(X), n_folds=10),\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "gs.fit(X, y)\n",
    "print gs.grid_scores_\n",
    "print (gs.best_estimator_)\n",
    "print (gs.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
