{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I will use the Twitter API Search through Tweepy to get the tweets containing the terms obtained from Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "\n",
    "try:\n",
    "    import json\n",
    "except ImportError:\n",
    "    import simplejson as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I set up my credentials for getting access to Twitter API - App 1\n",
    "ckey=\"oglAcoBXr2U41V3BRerO8SoiX\"\n",
    "csecret=\"o3MmdHBNQX5O5SV84N70IHkRCIPgOiVKNwIg1PQMznq1ZuvM8j\"\n",
    "atoken=\"848659773176385536-XOsIpQcOurrAYHE8qjnq6yq2gtJpw01\"\n",
    "asecret=\"gIazV001cMvvqtPdfJ1XyksnwSrednnNYQMb1z44PysPb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I define the function 'search_tweets', then to do the query we have to specify a'term' and a 'date'.\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "\n",
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "import csv\n",
    "\n",
    "# Date as YYYY-MM-DD\n",
    "def search_tweets(query,date):\n",
    "\n",
    "\t#authorize twitter, initialize tweepy\n",
    "\tauth = tweepy.OAuthHandler(ckey, csecret)\n",
    "\tauth.set_access_token(atoken, asecret)\n",
    "\tapi = tweepy.API(auth)\n",
    "\t\n",
    "\t#initialize a list to hold all the tweepy Tweets\n",
    "\talltweets = []\t\n",
    "\t\n",
    "\t#make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "\tnew_tweets = api.search(q = query,count=10, result_type = 'recent', include_entities =  True)\n",
    "\t\n",
    "\t#save most recent tweets\n",
    "\talltweets.extend(new_tweets)\n",
    "\t\n",
    "\t#save the id of the oldest tweet less one\n",
    "\toldest = alltweets[-1].id - 1\n",
    "\t\n",
    "\t#keep grabbing tweets until there are no more tweets to grab or the list reaches 2000, otherwise you get RateLimitError\n",
    "\twhile len(new_tweets) > 0 and len(alltweets) < 2000:\n",
    "\t\tprint \"getting tweets before %s\" % (oldest)\n",
    "\t\t\n",
    "\t\t#all subsiquent requests use the max_id param to prevent duplicates\n",
    "\t\tnew_tweets = api.search(q = query ,count=200,max_id=oldest, result_type = 'recent', include_entities = True)\n",
    "\t\t\n",
    "\t\t#save most recent tweets\n",
    "\t\talltweets.extend(new_tweets)\n",
    "\t\t\n",
    "\t\t#update the id of the oldest tweet less one\n",
    "\t\toldest = alltweets[-1].id - 1\n",
    "\t\t\n",
    "\t\tprint \"...%s tweets downloaded so far\" % (len(alltweets))   \n",
    "\t\n",
    "\t#transform the tweepy tweets into a 2D array that will populate the csv\t\n",
    "\touttweets = [[tweet.id_str, query, str(tweet.user.name.encode(\"utf-8\")), tweet.created_at, str(tweet.text.encode(\"utf-8\")).replace('\\n', ' ').replace('\\r', ''), tweet.favorite_count, tweet.retweet_count, tweet.user.followers_count] for tweet in alltweets]\n",
    "\t\n",
    "\t#write the csv\t\n",
    "# \twith open('/users/hnesman/Downloads/DATA_Science/Tweets_Bin/Tweets_Search_%s.csv' % (date) , 'wb') as f:\n",
    "# \twriter = csv.writer(f,delimiter ='|')\n",
    "\twriter.writerow([\"id\",\"query\",\"Username\",\"created_at\",\"text\",\"favorite_count\", \"retweet_count\",\"user's followers count\"])\n",
    "\twriter.writerows(outtweets)\n",
    "\t\n",
    "\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bochini\n",
      "getting tweets before 861892871762399231\n",
      "...110 tweets downloaded so far\n",
      "getting tweets before 861775918611738624\n",
      "...210 tweets downloaded so far\n",
      "getting tweets before 861738781061390335\n",
      "...310 tweets downloaded so far\n",
      "getting tweets before 861715345798823935\n",
      "...410 tweets downloaded so far\n",
      "getting tweets before 861685366910312447\n",
      "...510 tweets downloaded so far\n",
      "getting tweets before 861656905307107327\n",
      "...610 tweets downloaded so far\n",
      "getting tweets before 861637468449366015\n",
      "...710 tweets downloaded so far\n",
      "getting tweets before 861619814548336639\n",
      "...810 tweets downloaded so far\n",
      "getting tweets before 861605292819329024\n",
      "...896 tweets downloaded so far\n",
      "getting tweets before 861596300311224319\n",
      "...996 tweets downloaded so far\n",
      "getting tweets before 861586096198733823\n",
      "...1096 tweets downloaded so far\n",
      "getting tweets before 861575656920027135\n",
      "...1196 tweets downloaded so far\n",
      "getting tweets before 861561589358526467\n",
      "...1296 tweets downloaded so far\n",
      "getting tweets before 861545215991185407\n",
      "...1396 tweets downloaded so far\n",
      "getting tweets before 861527184086847487\n",
      "...1496 tweets downloaded so far\n",
      "getting tweets before 861469878053720064\n",
      "...1596 tweets downloaded so far\n",
      "getting tweets before 861445973243023359\n",
      "...1696 tweets downloaded so far\n",
      "getting tweets before 861439552808001536\n",
      "...1796 tweets downloaded so far\n",
      "getting tweets before 861437230090473471\n",
      "...1896 tweets downloaded so far\n",
      "getting tweets before 861435970981437439\n",
      "...1996 tweets downloaded so far\n",
      "getting tweets before 861386629679910911\n",
      "...2096 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "# Here I perform the search, 'query', 'date' are the arguments. I can put a list of terms in query, but no more than 5\n",
    "# can take per call, otherwise you get Rate Limited\n",
    "query = ['bochini']\n",
    "date = '2017-05-09'\n",
    "\n",
    "for i in query:\n",
    "    print i\n",
    "    f = open('/users/hnesman/Downloads/DATA_Science/Tweets_Bin/%s_Tweets_Search_%s.csv' % (i, date) , 'wb')\n",
    "    writer = csv.writer(f,delimiter ='|')\n",
    "    search_tweets(i,date)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
